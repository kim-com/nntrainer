[model]
batch_size = 32
continue_train = false
epochs = 200
loss = mse
loss_scale = 0.000000
memory_optimization = true
memory_swap = false
memory_swap_lookahead = 0
memory_swap_path = .
model_tensor_type = FP32-FP32
tensor_format = NCHW
type = NeuralNetwork

[optimizer]
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-07
learning_rate = 0.001000
torch_ref = false
type = adam

[rnn_in1]
input_shape = 1:1:12:13
loss_scale = 0.000000
name = rnn_in1
normalization = false
packed = true
standardization = false
trainable = true
type = input

[in2]
input_shape = 1:1:1:7
loss_scale = 0.000000
name = in2
normalization = false
packed = true
standardization = false
trainable = true
type = input

[rnn1]
bias_decay = 0.000000
bias_initializer = zeros
bidirectional = false
disable_bias = false
dropout_rate = 0.200000
hidden_state_activation = tanh
input_layers = rnn_in1(0)
integrate_bias = false
loss_scale = 0.000000
max_timestep = 12
name = rnn1
packed = true
print = false
recurrent_activation = sigmoid
return_sequences = false
trainable = true
type = lstm
unit = 64
weight_decay = 0.000000
weight_initializer = xavier_uniform
weight_regularizer = none
weight_regularizer_constant = 1.000000

[rnn2]
bias_decay = 0.000000
bias_initializer = zeros
bidirectional = false
disable_bias = false
dropout_rate = 0.200000
hidden_state_activation = tanh
input_layers = rnn1(0)
integrate_bias = false
loss_scale = 0.000000
max_timestep = 1
name = rnn2
packed = true
print = false
recurrent_activation = sigmoid
return_sequences = true
trainable = true
type = lstm
unit = 64
weight_decay = 0.000000
weight_initializer = xavier_uniform
weight_regularizer = none
weight_regularizer_constant = 1.000000

[rnn3]
bias_decay = 0.000000
bias_initializer = zeros
bidirectional = false
disable_bias = false
dropout_rate = 0.000000
hidden_state_activation = tanh
input_layers = rnn2(0)
integrate_bias = false
loss_scale = 0.000000
max_timestep = 1
name = rnn3
packed = true
print = false
recurrent_activation = sigmoid
return_sequences = false
trainable = true
type = lstm
unit = 64
weight_decay = 0.000000
weight_initializer = xavier_uniform
weight_regularizer = none
weight_regularizer_constant = 1.000000

[rnn3/activation_realized]
activation = relu
input_layers = rnn3(0)
loss_scale = 0.000000
name = rnn3/activation_realized
packed = true
trainable = true
type = activation

[cur_fc]
bias_decay = 0.000000
bias_initializer = zeros
disable_bias = false
input_layers = in2(0)
loss_scale = 0.000000
name = cur_fc
packed = true
print = false
trainable = true
type = fully_connected
unit = 64
weight_decay = 0.000000
weight_initializer = xavier_uniform
weight_regularizer = none
weight_regularizer_constant = 1.000000

[cur_fc/activation_realized]
activation = relu
input_layers = cur_fc(0)
loss_scale = 0.000000
name = cur_fc/activation_realized
packed = true
trainable = true
type = activation

[concat]
axis = 3
input_layers = cur_fc/activation_realized(0),rnn3/activation_realized(0)
loss_scale = 0.000000
name = concat
packed = true
trainable = true
type = concat

[output]
bias_decay = 0.000000
bias_initializer = zeros
disable_bias = false
input_layers = concat(0)
loss_scale = 0.000000
name = output
packed = true
print = false
trainable = true
type = fully_connected
unit = 1
weight_decay = 0.000000
weight_initializer = xavier_uniform
weight_regularizer = none
weight_regularizer_constant = 1.000000

